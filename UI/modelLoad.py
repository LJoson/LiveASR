# coding:utf-8

# from threading import Thread

from typing import Set
from typing import (List, Optional, TypedDict, Union)
from PySide6.QtCore import QThread, Signal
import sherpa_onnx
import os

"""
本模块使用了 sherpa-onnx 库进行语音识别模型的加载和推理。
sherpa-onnx 是一个基于 ONNX 的语音识别工具包，提供了高效的语音识别功能。
项目地址：https://github.com/k2-fsa/sherpa-onnx
"""

class modelParamDict(TypedDict):
    """
    Please refer to
    `<https://k2-fsa.github.io/sherpa/onnx/pretrained_models/index.html>`_
    to download pre-trained models for different languages, e.g., Chinese,
    English, etc.

    self:
        tokens:
        Path to ``tokens.txt``. Each line in ``tokens.txt`` contains two
        columns::

            symbol integer_id

        encoder:
        Path to ``encoder.onnx``.
        decoder:
        Path to ``decoder.onnx``.
        joiner:
        Path to ``joiner.onnx``.
        num_threads:
        Number of threads for neural network computation.
        sample_rate:
        Sample rate of the training data used to train the model.
        feature_dim:
        Dimension of the feature used to train the model.
        low_freq:
        Low cutoff frequency for mel bins in feature extraction.
        high_freq:
        High cutoff frequency for mel bins in feature extraction
        (if <= 0, offset from Nyquist)
        dither:
        Dithering constant (0.0 means no dither).
        By default the audio samples are in range [-1,+1],
        so dithering constant 0.00003 is a good value,
        equivalent to the default 1.0 from kaldi
        enable_endpoint_detection:
        True to enable endpoint detection. False to disable endpoint
        detection.
        rule1_min_trailing_silence:
        Used only when enable_endpoint_detection is True. If the duration
        of trailing silence in seconds is larger than this value, we assume
        an endpoint is detected.
        rule2_min_trailing_silence:
        Used only when enable_endpoint_detection is True. If we have decoded
        something that is nonsilence and if the duration of trailing silence
        in seconds is larger than this value, we assume an endpoint is
        detected.
        rule3_min_utterance_length:
        Used only when enable_endpoint_detection is True. If the utterance
        length in seconds is larger than this value, we assume an endpoint
        is detected.
        decoding_method:
        Valid values are greedy_search, modified_beam_search.
        max_active_paths:
        Use only when decoding_method is modified_beam_search. It specifies
        the maximum number of active paths during beam search.
        blank_penalty:
        The penalty applied on blank symbol during decoding.
        hotwords_file:
        The file containing hotwords, one words/phrases per line, and for each
        phrase the bpe/cjkchar are separated by a space.
        hotwords_score:
        The hotword score of each token for biasing word/phrase. Used only if
        hotwords_file is given with modified_beam_search as decoding method.
        temperature_scale:
        Temperature scaling for output symbol confidence estiamation.
        It affects only confidence values, the decoding uses the original
        logits without temperature.
        model_type:
        Online transducer model type. Valid values are: conformer, lstm,
        zipformer, zipformer2. All other values lead to loading the model twice.
        modeling_unit:
        The modeling unit of the model, commonly used units are bpe, cjkchar,
        cjkchar+bpe, etc. Currently, it is needed only when hotwords are
        provided, we need it to encode the hotwords into token sequence.
        bpe_vocab:
        The vocabulary generated by google's sentencepiece program.
        It is a file has two columns, one is the token, the other is
        the log probability, you can get it from the directory where
        your bpe model is generated. Only used when hotwords provided
        and the modeling unit is bpe or cjkchar+bpe.
        rule_fsts:
        If not empty, it specifies fsts for inverse text normalization.
        If there are multiple fsts, they are separated by a comma.
        rule_fars:
        If not empty, it specifies fst archives for inverse text normalization.
        If there are multiple archives, they are separated by a comma.
        provider:
        onnxruntime execution providers. Valid values are: cpu, cuda, coreml.
        device:
        onnxruntime cuda device index.
        cudnn_conv_algo_search:
        onxrt CuDNN convolution search algorithm selection. CUDA EP
        trt_max_workspace_size:
        Set TensorRT EP GPU memory usage limit. TensorRT EP
        trt_max_partition_iterations:
        Limit partitioning iterations for model conversion. TensorRT EP
        trt_min_subgraph_size:
        Set minimum size for subgraphs in partitioning. TensorRT EP
        trt_fp16_enable: bool = True,
        Enable FP16 precision for faster performance. TensorRT EP
        trt_detailed_build_log: bool = False,
        Enable detailed logging of build steps. TensorRT EP
        trt_engine_cache_enable: bool = True,
        Enable caching of TensorRT engines. TensorRT EP
        trt_timing_cache_enable: bool = True,
        "Enable use of timing cache to speed up builds." TensorRT EP
        trt_engine_cache_path: str ="",
        "Set path to store cached TensorRT engines." TensorRT EP
        trt_timing_cache_path: str ="",
        "Set path for storing timing cache." TensorRT EP
        trt_dump_subgraphs: bool = False,
        "Dump optimized subgraphs for debugging." TensorRT EP
    """
    decoder_model_size_or_path: str
    encoder_model_size_or_path : str
    joiner_model_size_or_path : str
    token_file_size_or_path : str
    rule_fsts_file_path: str
    hotwords_file_path: str
    hotwords_score: float
    device: str
    device_index: Union[int, List[int]]
    sample_rate: int
    # compute_type: str
    cpu_threads: int
    num_workers: int
    # download_root: Optional[str]
    # local_files_only: bool
    use_endpoint: bool

class LoadModelWorker(QThread):
    setStatusSignal = Signal(bool)
    loadModelOverSignal = Signal(bool)

    def __init__(self, modelParam: modelParamDict, parent = None):
        super().__init__(parent=parent)
        self.isRunning = False
        self.decoder_model_size_or_path: str = modelParam["decoder_model_size_or_path"]
        self.encoder_model_size_or_path: str = modelParam["encoder_model_size_or_path"]
        self.joiner_model_size_or_path: str = modelParam["joiner_model_size_or_path"]
        self.token_file_size_or_path: str = modelParam["token_file_size_or_path"]
        self.rule_fsts_file_path: str = modelParam["rule_fsts_file_path"]
        self.hotwords_file: str = modelParam["hotwords_file_path"]
        self.hotwords_score: float = modelParam["hotwords_score"]
        self.device: str = modelParam["device"]
        self.device_index: Union[int, List[int]] = modelParam["device_index"]
        self.sample_rate: int = modelParam["sample_rate"]
        # self.compute_type: str = modelParam["compute_type"]
        self.cpu_threads: int = modelParam["cpu_threads"]
        self.num_workers: int = modelParam["num_workers"]
        # self.download_root: Optional[str] = modelParam["download_root"]
        # self.local_files_only: bool = modelParam["local_files_only"]
        self.use_endpoint: bool = modelParam["use_endpoint"]
        self.decoding_method: str =""


        self.model = None

    def run(self) -> None:
        self.isRunning = True
        try:
            self.model = self.loadModel()
            self.setStatusSignal.emit(True)
        except Exception as e:
            print(f"加载模型时出错: {str(e)}")
            self.setStatusSignal.emit(False)
        finally:
            self.isRunning = False

    def stop(self):
        self.isRunning = False

    def loadModel(self, decoder_model_size_or_path:str=None) -> sherpa_onnx.OnlineRecognizer:
        model = None
        try:
            if not self.isRunning:
                return None

            if decoder_model_size_or_path is None:
                decoder_model_size_or_path = self.decoder_model_size_or_path

            if self.hotwords_file is not None:
                self.decoding_method = "modified_beam_search"

            # 检查所有必需的文件是否存在
            required_files = {
                'decoder': self.decoder_model_size_or_path,
                'encoder': self.encoder_model_size_or_path,
                'joiner': self.joiner_model_size_or_path,
                'tokens': self.token_file_size_or_path
            }

            for name, path in required_files.items():
                if not os.path.isfile(path):
                    print(f"错误：{name} 文件不存在: {path}")
                    return None

            model = sherpa_onnx.OnlineRecognizer.from_transducer(
                tokens=self.token_file_size_or_path,
                encoder=self.encoder_model_size_or_path,
                decoder=self.decoder_model_size_or_path,
                joiner=self.joiner_model_size_or_path,
                num_threads=self.num_workers,
                sample_rate=self.sample_rate,
                decoding_method=self.decoding_method,
                rule_fsts=self.rule_fsts_file_path,
                hotwords_score=self.hotwords_score,
                hotwords_file=self.hotwords_file,
                enable_endpoint_detection=self.use_endpoint,
                provider=self.device,
                device=self.device_index,
            )

            if model is not None:
                print("\n模型加载成功")
                self.setStatusSignal.emit(True)
            else:
                print("\n模型加载失败")
                self.setStatusSignal.emit(False)

        except Exception as e:
            print(f"\n加载模型时出错: {str(e)}")
            model = None
            self.setStatusSignal.emit(False)

        return model

